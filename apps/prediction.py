from sklearn.metrics import r2_score, mean_squared_error
from streamlit_option_menu import option_menu
from pathlib import Path
import statsmodels.api as sm
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from matplotlib import pyplot as plt
import streamlit as st
from sklearn.preprocessing import PolynomialFeatures
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score


@st.cache_data
def plot_regression_results(y_test, y_pred):
    # Функция для визуализации результатов линейной регрессии.
    # Построение графика фактических значений против предсказанных линейной регрессией
    fig = go.Figure()

    # Добавление истинных значений
    fig.add_trace(
        go.Scatter(
            x=y_test,
            y=y_pred,
            mode='markers',
            name='Predicted vs Actual',
            marker=dict(color='blue', size=10, opacity=0.5)
        )
    )

    # Линия идеального прогноза
    fig.add_trace(
        go.Scatter(
            x=[y_test.min(), y_test.max()],
            y=[y_test.min(), y_test.max()],
            mode='lines',
            name='Ideal Fit',
            line=dict(color='red', width=2)
        )
    )

    fig.update_layout(
        title='Actual vs. Predicted Values',
        xaxis_title='Actual Values',
        yaxis_title='Predicted Values',
        legend_title='Type',
        xaxis=dict(showline=True),
        yaxis=dict(showline=True)
    )

    # Построение гистограммы ошибок
    errors = y_test - y_pred
    fig_errors = go.Figure()
    fig_errors.add_trace(
        go.Histogram(
            x=errors,
            nbinsx=50,
            marker_color='blue'
        )
    )
    fig_errors.update_layout(
        title='Distribution of Prediction Errors',
        xaxis_title='Prediction Error',
        yaxis_title='Frequency',
        xaxis=dict(showline=True),
        yaxis=dict(showline=True)
    )
    st.plotly_chart(fig, use_container_width=True)
    st.plotly_chart(fig_errors, use_container_width=True)


def plot_predictions(
        timestamps,
        actual_values,
        predicted_values,
        title='Actual vs. Predicted'
):
    df_predictions = pd.DataFrame({
        'Timestamp': pd.to_datetime(timestamps),
        'Actual': actual_values,
        'Predicted': predicted_values
    })
    df_predictions = df_predictions.set_index('Timestamp').sort_index()
    fig = px.line(
        df_predictions,
        y=['Actual', 'Predicted'],
        markers=True,
        labels={'value': 'Bike Count', 'variable': 'Type'},
        title=title
    )
    st.plotly_chart(fig, use_container_width=True)


@st.cache_data
def plot_ape_mape(actual, pred):
    ape_values = np.abs((np.array(pred) - np.array(actual)) / np.array(actual)) * 100
    mape = np.mean(ape_values)

    fig = go.Figure()

    fig.add_trace(go.Bar(
        x=list(range(len(pred))),
        y=ape_values,
        name='APE',
        marker_color='blue'
    ))

    fig.add_trace(go.Scatter(
        x=list(range(len(pred))),
        y=[mape] * len(pred),
        mode='lines',
        name=f'MAPE = {mape:.3f}%',
        marker_color='red',
        line=dict(dash='dash', width=5)
    ))

    fig.update_layout(
        title='График APE с MAPE',
        template="plotly_white",
        xaxis_title='Номер точки данных',
        yaxis_title='APE (%)',
        legend_title="Легенда",
        font=dict(
            size=14,
        ),
    )
    st.plotly_chart(fig, use_container_width=True)


def create_plot(df, feature):
    fig = go.Figure(go.Scatter(
        x=df.index,
        y=df[feature],
        line=dict(width=3)
    ))

    fig.update_layout(
        title=f"Распределение {feature}",
        template="plotly_white",
        xaxis_title='Время',
        yaxis_title=feature,
        font=dict(size=12),
        xaxis=dict(
            tickangle=45,
            type='category',
            ticklabelstep=3
        )
    )
    return fig


@st.cache_data
def prepare_data(X, y):
    model = sm.OLS(y, X).fit()
    y_pred = model.predict(X)
    return model, y_pred


def calculate_metrics(y_true, y_pred):
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    mse = mean_squared_error(y_true, y_pred)
    r_squared = r2_score(y_true, y_pred)
    return mape, mse, r_squared


def app(df: pd.DataFrame, current_dir: Path):
    st.title("Прогнозирование использования городского велосипедного транспорта")
    st.markdown("""
        На этой странице представлен инструмент для прогнозирования количества велосипедных поездок в зависимости от различных факторов, таких как погода, время года и день недели.
        Вы можете выбрать параметры и увидеть предсказанное количество поездок.
   """)

    # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # ┃                     Разделение данных                     ┃
    # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    st.markdown("""
        ## Разделение и подготовка данных
        Разделим данные на обучающую и тестовую выборки, и определим переменные для линейной регрессии. 
        Для линейной регрессии мы выбираем переменные, которые предположительно имеют линейную связь с зависимой переменной (в данном случае price).
    """)
    # Для упрощения выберем несколько предикторов
    predictors = df.columns.drop('price')
    target = 'price'

    # Для модели используем только численные предикторы
    X_daily = df[predictors]
    y_daily = df[target]

    # Разделение данных на обучающую и тестовую выборки для месячных данных
    X_train, X_test, y_train, y_test = train_test_split(
        X_daily,
        y_daily,
        test_size=0.2,
        shuffle=False
    )
    tab1, tab2 = st.tabs(["Тренировочные данные", "Тестовые данные"])

    with tab1:
        st.subheader("Тренировочные данные")
        st.markdown("""
            **Описание:** Тренировочные данные используются для подгонки модели и оценки её параметров. 
            Эти данные получены путем исключения из исходного датасета столбцов с временными метками и целевой переменной 'price'.
            
            **Данные тренировочного набора (X_train)**.
            Обучающий набор данных содержит информацию о признаках, используемых для обучения модели. 
        """)
        st.dataframe(X_train)
        st.markdown("""
            **Целевая переменная (y_train)**.
            Целевая переменная содержит значения цены, которые модель должна научиться прогнозировать. 
            В качестве целевой переменной для тренировочного набора используются исключительно значения столбца 'price'.
        """)
        st.dataframe(pd.DataFrame(y_train).T)

        st.header("Визуализация числовых признаков")
        selected_feature = st.selectbox(
            "Выберите признак",
            list(X_train.columns),
            key="create_histogram_selectbox1"
        )
        st.plotly_chart(create_plot(X_train, selected_feature), use_container_width=True)

    with tab2:
        st.subheader("Тестовые данные")
        st.markdown("""
            **Описание:** Тестовые данные используются для проверки точности модели на данных, которые не участвовали в тренировке.
            Это позволяет оценить, как модель будет работать с новыми, ранее не виденными данными.
            """)
        st.markdown("""
            **Данные тестового набора (X_test)**.
            Тестовый набор данных содержит информацию о признаках, используемых для оценки модели.
        """)
        st.dataframe(X_test)
        st.markdown("""
            **Целевая переменная (y_test)**.
            Целевая переменная представляет собой значения, которые модель пытается предсказать. 
        """)
        st.dataframe(pd.DataFrame(y_test).T)
        st.header("Визуализация числовых признаков")
        selected_feature = st.selectbox(
            "Выберите признак",
            list(X_test.columns),
            key="create_histogram_selectbox2"
        )
        st.plotly_chart(create_plot(X_test, selected_feature), use_container_width=True)

    # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # ┃                     Моделирование                     ┃
    # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    st.header('Выбор типа математической модели прогноза')
    tab_linear, tab_poly = st.tabs(['Множественная линейная регрессия', 'Множественная полиноминальная регрессия'])
    # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # ┃                     Множественная линейная регрессия                     ┃
    # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    with tab_linear:
        st.markdown(r"""
            ## Множественная линейная регрессия
            Множественная линейная регрессия позволяет оценивать зависимость одной зависимой переменной от двух или более независимых переменных. Это делает её отличным инструментом для анализа и прогнозирования, где несколько факторов влияют на интересующий результат.
    
            ### Формула множественной линейной регрессии
            
            Формула множественной линейной регрессии выглядит следующим образом:
            
            $$
            y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_nx_n + \varepsilon 
            $$
            
            Где:
            - $ y $: Зависимая переменная (предсказываемая переменная). Это та переменная, значение которой мы пытаемся предсказать на основе независимых переменных.
            - $ \beta_0 $: Константа (интерцепт), представляющая собой значение $ y $, когда все независимые переменные равны нулю.
            - $ \beta_1, \beta_2, \ldots, \beta_n $: Коэффициенты независимых переменных, которые измеряют изменение зависимой переменной при изменении соответствующих независимых переменных.
            - $ x_1, x_2, \ldots, x_n $: Независимые переменные, используемые для предсказания значения $ y $.
            - $ \varepsilon $: Ошибка модели, описывающая разницу между наблюдаемыми значениями и значениями, предсказанными моделью.
            
            ### Описание параметров
            
            - **Зависимая переменная ( $ y $ )**: Это переменная, которую вы пытаетесь предсказать. Например, количество прокатов велосипедов может быть зависимой переменной, которую мы хотим предсказать на основе погоды, времени года и других условий.
              
            - **Константа ( $ \beta_0 $ )**: Это значение зависимой переменной, когда все входные (независимые) переменные равны нулю. В реальности это значение может не иметь физического смысла, особенно если ноль не является допустимым значением для независимых переменных.
            
            - **Коэффициенты ( $ \beta_1, \beta_2, \ldots, \beta_n $ )**: Эти значения указывают, насколько изменится зависимая переменная при изменении соответствующей независимой переменной на одну единицу, при условии что все остальные переменные остаются неизменными. Они являются ключевыми в понимании влияния каждой независимой переменной на зависимую переменную.
            
            - **Независимые переменные ( $ x_1, x_2, \ldots, x_n $ )**: Это переменные или факторы, которые предположительно влияют на зависимую переменную. В контексте вашего приложения это могут быть погода, день недели, сезон и другие.
            
            - **Ошибка модели ( $ \varepsilon $ )**: Ошибка модели показывает, насколько далеко наши предсказания от фактических значений. Это может быть вызвано неполным объяснением всех влияющих факторов или случайными изменениями, которые невозможно предсказать с помощью модели.
        """)

        linear_model, y_pred_train_linear = prepare_data(X_train, y_train)
        # Извлечение данных о параметрах модели
        st.subheader('Результаты модели')
        st.text(str(linear_model.summary())[:950])
        st.subheader('Коэффициенты модели')
        summary_data = linear_model.summary().tables[1]
        info = pd.DataFrame(summary_data.data[1:], columns=summary_data.data[0])
        st.dataframe(info, use_container_width=True, hide_index=True)

        # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
        # ┃                     Прогноз на зависимые данные                     ┃
        # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
        st.markdown("""
            ## Анализ точности построения модели
            Полиномиальная регрессия позволяет более точно описать зависимости в данных, что важно для прогнозирования в условиях изменчивой погоды и колебаний спроса на электроэнергию в отопительный период.
            ## Прогноз на зависимые данные
        """)

        mape, mse, r_squared = calculate_metrics(y_train, y_pred_train_linear.values)

        st.info(f"""
            ### Результаты прогноза на зависимые данные
            - **MAPE (Средняя абсолютная процентная ошибка):** {mape:.2f}%
            - **MSE (Среднеквадратичная ошибка):** {mse:.2f}
            - **R² (Коэффициент детерминации):** {r_squared:.3f}
        """)
        plot_regression_results(y_train, y_pred_train_linear)
        plot_predictions(
            y_train.index, y_train.values, y_pred_train_linear,
            'Тренировочные данные: фактическое и прогнозируемое количество велосипедов'
        )
        plot_ape_mape(y_train, y_pred_train_linear)

        # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
        # ┃                     Прогноз на независимые данные                     ┃
        # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
        y_pred_test_linear = linear_model.predict(X_test)
        mape, mse, r_squared = calculate_metrics(y_test, y_pred_test_linear.values)

        st.info(f"""
            ### Результаты прогноза на нeзависимые данные
            - **MAPE (Средняя абсолютная процентная ошибка):** {mape:.2f}%
            - **MSE (Среднеквадратичная ошибка):** {mse:.2f}
            - **R² (Коэффициент детерминации):** {r_squared:.3f}
        """)
        plot_regression_results(y_test, y_pred_test_linear)
        plot_predictions(
            y_test.index, y_test.values, y_pred_test_linear,
            'Тренировочные данные: фактическое и прогнозируемое количество велосипедов'
        )
        plot_ape_mape(y_test, y_pred_test_linear)

        st.markdown("""
            ### Выводы по результатам моделирования

            Множественная линейная регрессия показала себя как довольно надежный метод прогнозирования количества велосипедных поездок, что подтверждается значениями коэффициента детерминации $R²$. Однако существенные различия в показателях ошибок между зависимыми и независимыми данными указывают на возможные ограничения модели.
            
            Результаты на зависимых данных:
            - **MAPE 11.76%** и **MSE 24360.53** свидетельствуют о том, что модель достаточно точно предсказывает количество поездок на обучающей выборке. 
            - Значение **$R²$ равное 0.806** показывает, что около 80.6% вариабельности зависимой переменной объясняется независимыми переменными модели на обучающем наборе данных. Это свидетельствует о хорошей подгонке модели к данным.
            
            Результаты на независимых данных:
            - **MAPE 14.30%** и **MSE 36966.56** указывают на ухудшение точности прогнозов модели при переходе к тестовой выборке. Это может быть вызвано переобучением модели или недостаточной генерализацией способности модели к новым данным.
            - **$R²$ равное 0.730** на тестовой выборке все еще остается высоким, что означает, что модель объясняет 73% вариабельности зависимой переменной, но результаты менее убедительны по сравнению с обучающими данными.
            
            ### Выводы и рекомендации
            1. **Качество модели**: Модель эффективно описывает зависимости в данных, но показатели точности и коэффициент детерминации ухудшаются на независимых данных, что может указывать на переобучение.
            2. **Улучшение модели**:
               - **Перекрестная проверка**: Использование перекрестной проверки поможет оценить, насколько хорошо модель будет работать в общем случае, на различных подмножествах данных.
               - **Регуляризация**: Применение методов регуляризации, таких как Ridge или Lasso, может помочь уменьшить переобучение, ограничив влияние менее значимых переменных.
               - **Расширение данных**: Добавление дополнительных переменных, которые могут повлиять на количество поездок, например, данные о мероприятиях в городе или изменения в транспортной сети.
               - **Анализ выбросов**: Изучение и фильтрация выбросов в данных могут помочь улучшить общую производительность модели.
            
            3. **Мониторинг модели**: Регулярное обновление модели с учетом новых данных и тенденций поможет поддерживать ее актуальность и точность прогнозов.
            
            Эти меры позволят повысить точность и надежность прогнозов, а также сделать модель более устойчивой к изменениям в данных.
       """)

    # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # ┃                     Множественная полиноминальная регрессия                     ┃
    # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    with tab_poly:
        st.markdown(r"""
            ## Множественная полиноминальная регрессия
            Множественная полиноминальная регрессия представляет собой продвинутую версию линейной регрессии, предназначенную для изучения взаимосвязей между одной зависимой и несколькими независимыми переменными. Она выражается уравнением:
    
            $$
            y_t = \phi_0 + a_t + \sum_{i=1}^{n}{\beta_iz_i} + \sum_{j=1}^{m}{\varepsilon_jx_j^p} + \varepsilon_t
            $$
    
            Где:
            - $y_t$: Зависимая переменная (предсказываемая переменная).
            - $\phi_0$: Константа (Постоянный уровень ряда значение $y_t$, когда все независимые переменные равны нулю).
            - $a_t$: Тренд времени (Отражает как изменяется $y_t$ со временем. Может быть линейным, например, $a_t=\beta t$, или нелинейным, например, $a_t=\beta_1t+\beta_2t^2$).
            - Дамми переменные $z$: (обычно 0 или 1) используются для моделирования категориальных влияний на зависимую переменную.
            - Коэффициенты $\varepsilon_1,…,\varepsilon_m$: показывают величину влияния соответствующих независимых переменных на зависимую переменную.
            - Независимые переменные $x_1,…,x_m$: предполагаемые факторы, влияющие на $y_t$.
            - Полиномиальные члены: позволяют модели учитывать не только линейные, но и более сложные, нелинейные взаимосвязи между переменными.
            - Остатки модели $\varepsilon_t$: разница между наблюдаемыми значениями зависимой переменной и значениями, предсказанными моделью.
    
            ### Преимущества и недостатки:
            Множественная полиноминальная регрессия обладает способностью улавливать нелинейные связи и гибкостью в моделировании различных типов данных, что делает её мощным инструментом для анализа. Однако, такая модель может страдать от переобучения, проблем мультиколлинеарности и требует повышенных вычислительных ресурсов. Важно проводить регулярную оценку остатков для проверки адекватности модели и обновлять модель, включая новые данные для повышения точности прогнозов.
    
            Эта модель подходит для анализа сложных взаимосвязей, когда простая линейная регрессия оказывается недостаточной, и широко применяется в эконометрике, социальных науках и других областях.
        """)

        st.markdown("""
            ## Модель полиномиальной регрессии
            Модель полиномиальной регрессии используется для анализа нелинейных зависимостей между предикторами и ценой электроэнергии.
            Ниже представлены параметры модели, оценки коэффициентов, их стандартные ошибки, t-статистики и p-значения, что помогает оценить статистическую значимость каждого коэффициента.
        """)

        poly_features = PolynomialFeatures(degree=2)
        X_poly_train = poly_features.fit_transform(X_train)
        X_poly_test = poly_features.transform(X_test)

        ols_model, y_pred_train = prepare_data(X_poly_train, y_train)

        # Извлечение данных о параметрах модели
        summary_data = ols_model.summary().tables[1]
        info = pd.DataFrame(summary_data.data[1:], columns=summary_data.data[0])
        feature_names = poly_features.get_feature_names_out(X_train.columns)
        info['Param'] = ['Intercept' if i == 0 else feature_names[i] for i in range(len(feature_names))]

        st.subheader('Результаты модели')
        st.text(str(ols_model.summary())[:950])
        st.subheader('Коэффициенты модели')
        st.dataframe(info, use_container_width=True, hide_index=True)

        # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
        # ┃                     Прогноз на зависимые данные                     ┃
        # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
        st.markdown("""
            ## Анализ точности построения модели
            Полиномиальная регрессия позволяет более точно описать зависимости в данных, что важно для прогнозирования в условиях изменчивой погоды и колебаний спроса на электроэнергию в отопительный период.
            ## Прогноз на зависимые данные
        """)

        mape, mse, r_squared = calculate_metrics(y_train, y_pred_train)

        st.info(f"""
            ### Результаты прогноза на зависимые данные
            - **MAPE (Средняя абсолютная процентная ошибка):** {mape:.2f}%
            - **MSE (Среднеквадратичная ошибка):** {mse:.2f}
            - **R² (Коэффициент детерминации):** {r_squared:.3f}
        """)
        plot_regression_results(y_train, y_pred_train)
        plot_predictions(
            y_train.index, y_train.values, y_pred_train,
            'Тренировочные данные: фактическое и прогнозируемое количество велосипедов'
        )
        plot_ape_mape(y_train, y_pred_train)

        # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
        # ┃                     Прогноз на независимые данные                     ┃
        # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
        y_pred_test = ols_model.predict(X_poly_test)
        mape, mse, r_squared = calculate_metrics(y_test, y_pred_test)

        st.info(f"""
            ### Результаты прогноза на нeзависимые данные
            - **MAPE (Средняя абсолютная процентная ошибка):** {mape:.2f}%
            - **MSE (Среднеквадратичная ошибка):** {mse:.2f}
            - **R² (Коэффициент детерминации):** {r_squared:.3f}
        """)
        plot_regression_results(y_test, y_pred_test)
        plot_predictions(
            y_test.index, y_test.values, y_pred_test,
            'Тренировочные данные: фактическое и прогнозируемое количество велосипедов'
        )
        plot_ape_mape(y_test, y_pred_test)

        st.markdown("""
            ### Выводы по результатам моделирования
            Анализируя результаты прогноза с использованием множественной полиномиальной регрессии, можно сделать следующие выводы о модели и её производительности:
            
            Анализ результатов на зависимых данных:
            - **MAPE (7.80%)** и **MSE (13042.88)** показывают, что модель обладает высокой точностью прогнозирования на обучающих данных. Эти значения указывают на низкий уровень ошибок, что свидетельствует о хорошем соответствии модели данным.
            - **R² (0.896)** говорит о том, что почти 90% изменчивости зависимой переменной успешно объясняется переменными модели. Это отличный результат, который показывает, что модель хорошо подходит для данных.
            
            Анализ результатов на независимых данных:
            - **MAPE (17.72%)** и **MSE (57767.88)** значительно выше, чем на обучающих данных. Это увеличение ошибок на тестовых данных может указывать на переобучение модели.
            - **R² (0.577)** значительно снижается по сравнению с обучающим набором, что свидетельствует о потере точности модели при работе с новыми данными. Этот показатель указывает на то, что только около 57.7% вариативности данных может быть объяснено моделью в условиях независимого тестирования.
            
            ### Выводы
            1. **Потенциальное переобучение**: Разница в результатах между обучающими и тестовыми данными указывает на переобучение модели. Модель, которая отлично работает на обучающих данных, может не справляться с новыми, неизвестными данными.
            2. **Необходимость регуляризации**: Для снижения риска переобучения можно применить методы регуляризации, такие как Lasso или Ridge, которые помогут сократить влияние менее важных признаков.
            3. **Использование валидационного набора данных**: Введение валидационного набора данных может помочь лучше оценить производительность модели до её тестирования, что позволит более точно настроить параметры модели.
            4. **Проверка модели на стабильность**: Использование методов перекрёстной проверки может помочь проверить стабильность модели на различных подвыборках данных и уменьшить вариативность прогнозов.
            5. **Оценка значимости переменных**: Полиномиальная регрессия может привести к включению большого числа переменных, не все из которых значимы. Оценка значимости переменных поможет сократить количество признаков, сохраняя только те, которые действительно важны для модели.
            Эти меры помогут улучшить точность модели и её способность обобщать результаты на новых данных, делая её более надёжной для прогнозирования использования городского велосипедного транспорта.
        """)

    # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # ┃                     Форма ввода                     ┃
    # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    with st.form("Форма ввода для прогнозирования использования велосипедов"):
        st.subheader('Введите параметры для прогноза')
        inputs = pd.DataFrame(columns=X_test.columns)

        numerical_inputs = {
            't1'           : "Реальная температура в градусах Цельсия",
            't2'           : "Ощущаемая температура в градусах Цельсия",
            'hum'          : "Влажность в процентах",
            'wind_speed'   : "Скорость ветра в км/ч"
        }
        weather_options = {
            "Ясно": 1, "Рассеянные облака": 2,
            "Разорванные облака": 3, "Облачно": 4, "Дождь": 7,
            "Дождь с грозой": 10, "Снегопад": 26, "Морозный туман": 94
        }
        season_options = {
            'Весна': 0, 'Лето': 1, 'Осень': 2, 'Зима': 3
        }
        month_options = {
            'Январь': 'Jan', 'Февраль': 'Feb', 'Март': 'Mar', 'Апрель': 'Apr', 'Май': 'May', 'Июнь': 'June',
            'Июль': 'July', 'Август': 'Aug', 'Сентябрь': 'Sep', 'Октябрь': 'Oct', 'Ноябрь': 'Nov', 'Декабрь': 'Dec'
        }
        weekday_options = {
            'Понедельник': 'Monday', 'Вторник': 'Tuesday', 'Среда': 'Wednesday',
            'Четверг': 'Thursday', 'Пятница': 'Friday', 'Суббота': 'Saturday', 'Воскресенье': 'Sunday'
        }
        for feature in numerical_inputs:
            inputs[feature] = [
                st.number_input(
                    label=numerical_inputs[feature],
                    min_value=float(df[feature].min()-abs(df[feature].max()*2)),
                    max_value=float(df[feature].max() * 3),
                    value=float(df[feature].mean()),
                    step=0.01,
                    key=f'num_{feature}'
                )
            ]
        col1, col2 = st.columns(2)
        with col1:
            weather_code = st.selectbox(
                "Погодные условия",
                list(weather_options.keys()),
                index=0,
                key=f'cat_weather_code'
            )
            inputs['weather_code'] = weather_options[weather_code]

            is_holiday = option_menu(
                'Праздничный день?',
                options=['Нет', 'Да'],
                icons=["circle", "check2-circle"],
                default_index=1,
                key=f'cat_is_holiday'
            )
            inputs['is_holiday'] = int(is_holiday == 'Да')

            weekday = st.selectbox(
                "День недели",
                list(weekday_options.keys()),
                index=0,
                key='cat_weekday'
            )
            for key in weekday_options.values():
                inputs[key] = int(key == weekday_options[ weekday])

        with col2:
            season = st.selectbox(
                "Сезон",
                list(season_options.keys()),
                index=1,
                key=f'cat_season'
            )
            inputs['season'] = season_options[season]

            is_weekend = option_menu(
                'Выходной день?',
                options=['Нет', 'Да'],
                icons=["circle", "check2-circle"],
                default_index=1,
                key='cat_is_weekend'
            )
            inputs['is_weekend'] = int(is_weekend == 'Да')

            month = st.selectbox(
                "Месяц",
                list(month_options.keys()),
                index=6,
                key='cat_month'
            )
            for key in month_options.values():
                inputs[key] = int(key == month_options[month])

        if st.form_submit_button("Прогнозировать", type='primary', use_container_width=True):
            # input_df = decode_inputs(inputs, numerical_features[1:], hours_features)
            # input_df = pd.DataFrame([inputs])
            prediction = max(ols_model.predict(poly_features.transform(inputs))[0], 0)
            st.success("Прогноз успешно выполнен!")

            fig = go.Figure(go.Indicator(
                mode="number",
                value=prediction,
                number={"valueformat": ".0f"},
                title={"text": "Прогнозируемое потребление"}
            ))

            fig.update_layout(paper_bgcolor="#f0f2f6", font={'color': "darkblue", 'family': "Arial"})
            st.plotly_chart(fig, use_container_width=True)
