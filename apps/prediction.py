import plotly.figure_factory as ff
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import r2_score, mean_squared_error
from streamlit_option_menu import option_menu
from pathlib import Path
import statsmodels.api as sm
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from matplotlib import pyplot as plt
import streamlit as st
from sklearn.preprocessing import PolynomialFeatures
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score


@st.cache_data
def plot_regression_results(y_test, y_pred):
    # Функция для визуализации результатов линейной регрессии.
    # Построение графика фактических значений против предсказанных линейной регрессией
    fig = go.Figure()

    # Добавление истинных значений
    fig.add_trace(
        go.Scatter(
            x=y_test,
            y=y_pred,
            mode='markers',
            name='Predicted vs Actual',
            marker=dict(color='blue', size=10, opacity=0.5)
        )
    )

    # Линия идеального прогноза
    fig.add_trace(
        go.Scatter(
            x=[y_test.min(), y_test.max()],
            y=[y_test.min(), y_test.max()],
            mode='lines',
            name='Ideal Fit',
            line=dict(color='red', width=2)
        )
    )

    fig.update_layout(
        title='Actual vs. Predicted Values',
        xaxis_title='Actual Values',
        yaxis_title='Predicted Values',
        legend_title='Type',
        xaxis=dict(showline=True),
        yaxis=dict(showline=True)
    )

    # Построение гистограммы ошибок
    errors = y_test - y_pred
    fig_errors = go.Figure()
    fig_errors.add_trace(
        go.Histogram(
            x=errors,
            nbinsx=50,
            marker_color='blue'
        )
    )
    fig_errors.update_layout(
        title='Distribution of Prediction Errors',
        xaxis_title='Prediction Error',
        yaxis_title='Frequency',
        xaxis=dict(showline=True),
        yaxis=dict(showline=True)
    )
    st.plotly_chart(fig, use_container_width=True)
    st.plotly_chart(fig_errors, use_container_width=True)


@st.cache_data
def prepare_data(X, y):
    model = sm.OLS(y, X).fit()
    y_pred = model.predict(X)
    return model, y_pred


@st.cache_data
def create_correlation_matrix(df):
    corr = df.corr().round(2)
    fig = ff.create_annotated_heatmap(
        z=corr.values,
        x=list(corr.columns),
        y=list(corr.index),
        colorscale='greens',
        annotation_text=corr.values
    )
    fig.update_layout(height=500)
    return fig


def calculate_metrics(y_true, y_pred):
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    mse = mean_squared_error(y_true, y_pred)
    r_squared = r2_score(y_true, y_pred)
    return mape, mse, r_squared


def encode_features(df, features):
    encoders = {}
    for feature in features:
        le = LabelEncoder()
        df[feature] = le.fit_transform(df[feature])
        encoders[feature] = le
    return df, encoders



def app(df: pd.DataFrame, current_dir: Path):
    st.title("Анализ и прогнозирование цен на подержанные автомобили")
    st.markdown("""
          На этой странице представлен инструмент для анализа и прогнозирования цен на подержанные автомобили. Используя данные о продажах автомобилей, можно выявить ключевые факторы, влияющие на стоимость, и определить наиболее важные характеристики, которые влияют на цену.
          Вы можете загрузить свои данные, выбрать параметры и увидеть анализ и прогнозируемую цену.
      """)
    # Замена пропущенных значений в категориальных данных
    categorical_columns = ['vehicleType', 'gearbox', 'model', 'fuelType', 'notRepairedDamage']
    autos_encoded = df.copy()

    # Применение LabelEncoding ко всем категориальным столбцам
    df, encoders = encode_features(df, categorical_columns)
    # label_encoder = LabelEncoder()
    # for column in categorical_columns:
    #     df[column] = label_encoder.fit_transform(df[column])


    # Удаление колонок, не несущих значимой информации для анализа
    autos_final = df.drop(columns=[
        'dateCrawled', 'lastSeen', 'postalCode', 'dateCreated', 'name', 'abtest', 'brand'
    ])
    autos_final = autos_final.reindex(sorted(autos_final.columns), axis=1)

    st.dataframe(autos_final, use_container_width=True)
    # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # ┃                     Разделение данных                     ┃
    # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    st.markdown("""
        ## Разделение и подготовка данных
        **Разделим данные** на обучающую и тестовую выборки, и определим переменные, которые используем в модели прогнозирования. 
        Для этого выберем количественные переменные, а также преобразуем категориальные переменные для включения их в модель.
    """)
    st.markdown("""
        ## Анализ корреляции между переменными
        Понимание корреляции между различными переменными помогает выявить, какие факторы наиболее сильно влияют на ценообразование.
    """)
    st.plotly_chart(create_correlation_matrix(autos_final), use_container_width=True)

    # Для упрощения выберем несколько предикторов
    predictors = autos_final.columns.drop('price')
    target = 'price'

    # Разделение данных на обучающую и тестовую выборки для месячных данных
    X_train, X_test, y_train, y_test = train_test_split(
        autos_final[predictors],
        autos_final[target],
        test_size=0.2,
        shuffle=False
    )
    # tab1, tab2 = st.tabs(["Тренировочные данные", "Тестовые данные"])
    #
    # with tab1:
    #     st.subheader("Тренировочные данные")
    #     st.markdown("""
    #         **Описание:** Тренировочные данные используются для подгонки модели и оценки её параметров.
    #         Эти данные получены путем исключения из исходного датасета столбцов с временными метками и целевой переменной 'price'.
    #
    #         **Данные тренировочного набора (X_train)**.
    #         Обучающий набор данных содержит информацию о признаках, используемых для обучения модели.
    #     """)
    #     st.dataframe(X_train, use_container_width=True)
    #     st.markdown("""
    #         **Целевая переменная (y_train)**.
    #         Целевая переменная содержит значения цены, которые модель должна научиться прогнозировать.
    #         В качестве целевой переменной для тренировочного набора используются исключительно значения столбца 'price'.
    #     """)
    #     st.dataframe(pd.DataFrame(y_train).T)
    #
    # with tab2:
    #     st.subheader("Тестовые данные")
    #     st.markdown("""
    #         **Описание:** Тестовые данные используются для проверки точности модели на данных, которые не участвовали в тренировке.
    #         Это позволяет оценить, как модель будет работать с новыми, ранее не виденными данными.
    #         """)
    #     st.markdown("""
    #         **Данные тестового набора (X_test)**.
    #         Тестовый набор данных содержит информацию о признаках, используемых для оценки модели.
    #     """)
    #     st.dataframe(X_test, use_container_width=True)
    #     st.markdown("""
    #         **Целевая переменная (y_test)**.
    #         Целевая переменная представляет собой значения, которые модель пытается предсказать.
    #     """)
    #     st.dataframe(pd.DataFrame(y_test).T)
    #
    # # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # # ┃                     Моделирование                     ┃
    # # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    # st.header('Выбор типа математической модели прогноза')
    # tab_poly, tab_linear = st.tabs(['Множественная полиноминальная регрессия', 'Множественная линейная регрессия'])
    # # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # # ┃                     Множественная линейная регрессия                     ┃
    # # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    #
    # # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # # ┃                     Множественная полиноминальная регрессия                     ┃
    # # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    # with tab_poly:
    st.markdown(r"""
        ## Множественная полиноминальная регрессия
        Множественная полиноминальная регрессия представляет собой продвинутую версию линейной регрессии, предназначенную для изучения взаимосвязей между одной зависимой и несколькими независимыми переменными. Она выражается уравнением:

        $$
        y_t = \phi_0 + a_t + \sum_{i=1}^{n}{\beta_iz_i} + \sum_{j=1}^{m}{\varepsilon_jx_j^p} + \varepsilon_t
        $$

        Где:
        - $y_t$: Зависимая переменная (предсказываемая переменная).
        - $\phi_0$: Константа (Постоянный уровень ряда значение $y_t$, когда все независимые переменные равны нулю).
        - $a_t$: Тренд времени (Отражает как изменяется $y_t$ со временем. Может быть линейным, например, $a_t=\beta t$, или нелинейным, например, $a_t=\beta_1t+\beta_2t^2$).
        - Дамми переменные $z$: (обычно 0 или 1) используются для моделирования категориальных влияний на зависимую переменную.
        - Коэффициенты $\varepsilon_1,…,\varepsilon_m$: показывают величину влияния соответствующих независимых переменных на зависимую переменную.
        - Независимые переменные $x_1,…,x_m$: предполагаемые факторы, влияющие на $y_t$.
        - Полиномиальные члены: позволяют модели учитывать не только линейные, но и более сложные, нелинейные взаимосвязи между переменными.
        - Остатки модели $\varepsilon_t$: разница между наблюдаемыми значениями зависимой переменной и значениями, предсказанными моделью.

        ### Преимущества и недостатки:
        Множественная полиноминальная регрессия обладает способностью улавливать нелинейные связи и гибкостью в моделировании различных типов данных, что делает её мощным инструментом для анализа. Однако, такая модель может страдать от переобучения, проблем мультиколлинеарности и требует повышенных вычислительных ресурсов. Важно проводить регулярную оценку остатков для проверки адекватности модели и обновлять модель, включая новые данные для повышения точности прогнозов.

        Эта модель подходит для анализа сложных взаимосвязей, когда простая линейная регрессия оказывается недостаточной, и широко применяется в эконометрике, социальных науках и других областях.
    """)

    st.markdown("""
        ## Модель полиномиальной регрессии
        Модель полиномиальной регрессии используется для анализа нелинейных зависимостей между предикторами и ценой электроэнергии.
        Ниже представлены параметры модели, оценки коэффициентов, их стандартные ошибки, t-статистики и p-значения, что помогает оценить статистическую значимость каждого коэффициента.
    """)

    poly_features = PolynomialFeatures(degree=2)
    X_poly_train = poly_features.fit_transform(X_train)
    X_poly_test = poly_features.transform(X_test)

    ols_model, y_pred_train = prepare_data(X_poly_train, y_train)

    # Извлечение данных о параметрах модели
    summary_data = ols_model.summary().tables[1]
    info = pd.DataFrame(summary_data.data[1:], columns=summary_data.data[0])
    feature_names = poly_features.get_feature_names_out(X_train.columns)
    info['Param'] = ['Intercept' if i == 0 else feature_names[i] for i in range(len(feature_names))]

    st.subheader('Результаты модели')
    st.text(str(ols_model.summary())[:950])
    st.subheader('Коэффициенты модели')
    st.dataframe(info, use_container_width=True, hide_index=True)

    # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # ┃                     Прогноз на зависимые данные                     ┃
    # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    st.markdown("""
        ## Анализ точности построения модели
        Полиномиальная регрессия позволяет более точно описать зависимости в данных, что важно для прогнозирования в условиях изменчивой погоды и колебаний спроса на электроэнергию в отопительный период.
        ## Прогноз на зависимые данные
    """)

    mape, mse, r_squared = calculate_metrics(y_train, y_pred_train)

    st.info(f"""
        ### Результаты прогноза на зависимые данные
        - **MAPE (Средняя абсолютная процентная ошибка):** {mape:.2f}%
        - **MSE (Среднеквадратичная ошибка):** {mse:.2f}
        - **R² (Коэффициент детерминации):** {r_squared:.3f}
    """)
    plot_regression_results(y_train, y_pred_train)
    # plot_predictions(
    #     y_train.index, y_train.values, y_pred_train,
    #     'Тренировочные данные: фактическое и прогнозируемое количество велосипедов'
    # )
    # plot_ape_mape(y_train, y_pred_train)

    # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # ┃                     Прогноз на независимые данные                     ┃
    # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    y_pred_test = ols_model.predict(X_poly_test)
    mape, mse, r_squared = calculate_metrics(y_test, y_pred_test)

    st.info(f"""
        ### Результаты прогноза на нeзависимые данные
        - **MAPE (Средняя абсолютная процентная ошибка):** {mape:.2f}%
        - **MSE (Среднеквадратичная ошибка):** {mse:.2f}
        - **R² (Коэффициент детерминации):** {r_squared:.3f}
    """)
    plot_regression_results(y_test, y_pred_test)
    # plot_predictions(
    #     y_test.index, y_test.values, y_pred_test,
    #     'Тренировочные данные'
    # )
    # plot_ape_mape(y_test, y_pred_test)

    st.markdown("""
        ### Выводы по результатам моделирования
        * Средняя абсолютная ошибка (MAE): Это среднее значение абсолютных различий между фактическими и предсказанными значениями. В нашем случае MAE составляет примерно 1841 единицу.
        * Средняя абсолютная процентная ошибка (MAPE): Эта метрика измеряет относительную ошибку в процентах. Здесь ошибка составляет около 90.10%.
        * Корень из среднеквадратичной ошибки (RMSE): Это стандартное отклонение остатков модели. RMSE равен примерно 3096.78 единицам.
        Коэффициент детерминации (R^2): Он измеряет долю дисперсии зависимой переменной, которая объясняется моделью. Значение около 0.76 означает, что примерно 76% дисперсии объясняется независимыми переменными, включенными в модель.

        Исходя из этих метрик, можно сделать вывод, что полиномиальная регрессионная модель дает более точные предсказания, чем предыдущая модель. Она имеет более низкие значения MAE и RMSE, что свидетельствует о более точном соответствии между фактическими и предсказанными значениями. Кроме того, значение R^2 ближе к 1, что указывает на более высокую объяснительную способность модели.
    """)
    #
    # with tab_linear:
    #     st.markdown(r"""
    #         ## Множественная линейная регрессия
    #         Множественная линейная регрессия позволяет оценивать зависимость одной зависимой переменной от двух или более независимых переменных. Это делает её отличным инструментом для анализа и прогнозирования, где несколько факторов влияют на интересующий результат.
    #
    #         ### Формула множественной линейной регрессии
    #
    #         Формула множественной линейной регрессии выглядит следующим образом:
    #
    #         $$
    #         y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_nx_n + \varepsilon
    #         $$
    #
    #         Где:
    #         - $ y $: Зависимая переменная (предсказываемая переменная). Это та переменная, значение которой мы пытаемся предсказать на основе независимых переменных.
    #         - $ \beta_0 $: Константа (интерцепт), представляющая собой значение $ y $, когда все независимые переменные равны нулю.
    #         - $ \beta_1, \beta_2, \ldots, \beta_n $: Коэффициенты независимых переменных, которые измеряют изменение зависимой переменной при изменении соответствующих независимых переменных.
    #         - $ x_1, x_2, \ldots, x_n $: Независимые переменные, используемые для предсказания значения $ y $.
    #         - $ \varepsilon $: Ошибка модели, описывающая разницу между наблюдаемыми значениями и значениями, предсказанными моделью.
    #
    #         ### Описание параметров
    #
    #         - **Зависимая переменная ( $ y $ )**: Это переменная, которую вы пытаетесь предсказать. Например, количество прокатов велосипедов может быть зависимой переменной, которую мы хотим предсказать на основе погоды, времени года и других условий.
    #
    #         - **Константа ( $ \beta_0 $ )**: Это значение зависимой переменной, когда все входные (независимые) переменные равны нулю. В реальности это значение может не иметь физического смысла, особенно если ноль не является допустимым значением для независимых переменных.
    #
    #         - **Коэффициенты ( $ \beta_1, \beta_2, \ldots, \beta_n $ )**: Эти значения указывают, насколько изменится зависимая переменная при изменении соответствующей независимой переменной на одну единицу, при условии что все остальные переменные остаются неизменными. Они являются ключевыми в понимании влияния каждой независимой переменной на зависимую переменную.
    #
    #         - **Независимые переменные ( $ x_1, x_2, \ldots, x_n $ )**: Это переменные или факторы, которые предположительно влияют на зависимую переменную. В контексте вашего приложения это могут быть погода, день недели, сезон и другие.
    #
    #         - **Ошибка модели ( $ \varepsilon $ )**: Ошибка модели показывает, насколько далеко наши предсказания от фактических значений. Это может быть вызвано неполным объяснением всех влияющих факторов или случайными изменениями, которые невозможно предсказать с помощью модели.
    #     """)
    #
    #     linear_model, y_pred_train_linear = prepare_data(X_train, y_train)
    #     # Извлечение данных о параметрах модели
    #     st.subheader('Результаты модели')
    #     st.text(str(linear_model.summary())[:950])
    #     st.subheader('Коэффициенты модели')
    #     summary_data = linear_model.summary().tables[1]
    #     info = pd.DataFrame(summary_data.data[1:], columns=summary_data.data[0])
    #     st.dataframe(info, use_container_width=True, hide_index=True)
    #
    #     # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    #     # ┃                     Прогноз на зависимые данные                     ┃
    #     # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    #     st.markdown("""
    #         ## Анализ точности построения модели
    #         ## Прогноз на зависимые данные
    #     """)
    #
    #     mape, mse, r_squared = calculate_metrics(y_train, y_pred_train_linear.values)
    #
    #     st.info(f"""
    #         ### Результаты прогноза на зависимые данные
    #         - **MAPE (Средняя абсолютная процентная ошибка):** {mape:.2f}%
    #         - **MSE (Среднеквадратичная ошибка):** {mse:.2f}
    #         - **R² (Коэффициент детерминации):** {r_squared:.3f}
    #     """)
    #     plot_regression_results(y_train, y_pred_train_linear)
    #
    #     # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    #     # ┃                     Прогноз на независимые данные                     ┃
    #     # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    #     y_pred_test_linear = linear_model.predict(X_test)
    #     mape, mse, r_squared = calculate_metrics(y_test, y_pred_test_linear.values)
    #
    #     st.info(f"""
    #         ### Результаты прогноза на нeзависимые данные
    #         - **MAPE (Средняя абсолютная процентная ошибка):** {mape:.2f}%
    #         - **MSE (Среднеквадратичная ошибка):** {mse:.2f}
    #         - **R² (Коэффициент детерминации):** {r_squared:.3f}
    #     """)
    #     plot_regression_results(y_test, y_pred_test_linear)
    #
    #     st.markdown("""
    #         ### Выводы по результатам моделирования
    #
    #         * Средняя абсолютная ошибка (MAE): Это средняя разница между предсказанными и реальными значениями. В нашем случае она составляет около 2566 единиц.
    #
    #         * Средняя абсолютная процентная ошибка (MAPE): Она выражает относительную ошибку в процентах. Здесь ошибка составляет около 172.77%.
    #
    #         * Корень из среднеквадратичной ошибки (RMSE): Это среднеквадратичное отклонение между фактическими и предсказанными значениями. Он равен примерно 3704.73 единицам.
    #
    #         * Коэффициент детерминации (R^2): Этот коэффициент показывает, как хорошо модель объясняет изменчивость зависимой переменной. Значение около 0.62 означает, что примерно 62% этой изменчивости объясняется независимыми переменными, включенными в модель.
    #
    #         Исходя из этих метрик, мы можем сделать вывод, что наша модель дает хорошие предсказания, но, вероятно, ее можно еще улучшить. Необходимо более детально проанализировать данные и возможно внести изменения в модель, чтобы добиться более точных результатов.
    #    """)

    # ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    # ┃                     Форма ввода                     ┃
    # ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
    # print(X_test)
    with st.form("Ввод данных о подержанных автомобилях"):
        st.subheader('Введите параметры для прогноза')
        year = st.number_input("Год регистрации", min_value=1980, max_value=2021, value=1998)
        power = st.number_input("Мощность (PS)", min_value=50, max_value=500, value=101)
        odometer = st.number_input("Пробег", min_value=0, max_value=300000, value=150000, step=1000)
        month = st.select_slider("Месяц регистрации", options=list(range(1, 13)), value=11)
        inputs = {}
        for column in categorical_columns:
            selection = st.selectbox(f"Выберите {column}", options=autos_encoded[column].unique(), index=0)
            inputs[column] = selection  # Вводим значения напрямую в текстовом виде

        if st.form_submit_button("Прогнозировать", type='primary', use_container_width=True):
            # Создаем строку с данными для декодирования
            encoded_inputs = {col: encoders[col].transform([inputs[col]])[0] for col in categorical_columns}
            all_inputs = pd.Series(
                {**encoded_inputs, 'yearOfRegistration': year, 'powerPS': power, 'odometer': odometer,
                 'monthOfRegistration': month})
            input_df = pd.DataFrame([all_inputs])
            input_df = input_df.reindex(sorted(X_test.columns), axis=1)
            prediction = max(ols_model.predict(poly_features.transform(input_df))[0], 0)
            st.success(f"Прогноз успешно выполнен! Предполагаемая стоимость: ${prediction:.2f}")

            fig = go.Figure(go.Indicator(
                mode="number",
                value=prediction,
                number={"valueformat": ".0f"},
                title={"text": "Прогнозируемая стоимость"}
            ))

            fig.update_layout(paper_bgcolor="#f0f2f6", font={'color': "darkblue", 'family': "Arial"})
            st.plotly_chart(fig, use_container_width=True)
